<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Hammad Ayyubi | publications</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Open Graph -->


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
        
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Hammad</span> Ayyubi</a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/alumni/">
                alumni
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/Ayyubi_Hammad_Resume.pdf">
              vitae
              
            </a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2024</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/journey_bench.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="wang2024journey" class="col-sm-7">
    
      <span class="title">JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images</span>
      <span class="author">
        
          
            
	    
                
                  Zhecan Wang,
                
              
            
          
        
          
            
	    
                
                  Junzhang Liu,
                
              
            
          
        
          
            
	    
                
                  Chia-Wei Tang,
                
              
            
          
        
          
            
	    
                
                  Hani Alomari,
                
              
            
          
        
          
            
	    
                
                  Anushka Sivakumar,
                
              
            
          
        
          
            
	    
                
                  Rui Sun,
                
              
            
          
        
          
            
	    
                
                  Wenhao Li,
                
              
            
          
        
          
            
	    
                
                  Md Atabuzzaman,
                
              
            
          
        
          
            
	    
                <em>Hammad Ayyubi</em>,
              
            
          
        
          
            
	    
                
                  Haoxuan You,
                
              
            
          
        
          
            
	    
                
                  Alvi Ishmam,
                
              
            
          
        
          
            
	    
                
                  Kai-Wei Chang,
                
              
            
          
        
          
            
	    
                
                  Shih-Fu Chang,
                
              
            
          
        
          
            
	    
                
                  and Chris Thomas
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>NeurIPS</em>
      
      
        2024
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/pdf/2409.12953" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Existing vision-language understanding benchmarks largely consist of images of objects in their usual contexts. As a consequence, recent multimodal large language models can perform well with only a shallow visual understanding by relying on background language biases. Thus, strong performance on these benchmarks does not necessarily correlate with strong visual understanding. In this paper, we release JourneyBench, a comprehensive human-annotated benchmark of generated images designed to assess the model’s fine-grained multimodal reasoning abilities across five tasks: complementary multimodal chain of thought, multi-image VQA, imaginary image captioning, VQA with hallucination triggers, and fine-grained retrieval with sample-specific distractors. Unlike existing benchmarks, JourneyBench explicitly requires fine-grained multimodal reasoning in unusual imaginary scenarios where language bias and holistic image gist are insufficient. We benchmark state-of-the-art models on JourneyBench and analyze performance along a number of fine-grained dimensions. Results across all five tasks show that JourneyBench is exceptionally challenging for even the best models, indicating that models’ visual reasoning abilities are not as strong as they first appear. We discuss the implications of our findings and propose avenues for further research.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/video_entity_sum.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="ayyubi2023videoentitysum" class="col-sm-7">
    
      <span class="title">
Video Summarization: Towards Entity-Aware Captions</span>
      <span class="author">
        
          
            
	    
                <em>Hammad A. Ayyubi</em>,
              
            
          
        
          
            
	    
                
                  Tianqi Liu,
                
              
            
          
        
          
            
	    
                
                  Arsha Nagrani,
                
              
            
          
        
          
            
	    
                
                  Xudong Lin,
                
              
            
          
        
          
            
	    
                
                  Mingda Zhang,
                
              
            
          
        
          
            
	    
                
                  Anurag Arnab,
                
              
            
          
        
          
            
	    
                
                  Feng Han,
                
              
            
          
        
          
            
	    
                
                  Yukun Zhu,
                
              
            
          
        
          
            
	    
                
                  Jialu Liu,
                
              
            
          
        
          
            
	    
                
                  and Shih-Fu Chang
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>EMNLP</em>
      
      
        2024
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2312.02188" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Existing popular video captioning benchmarks and models deal with generic captions devoid of specific person, place or organization named entities. In contrast, news videos present a challenging setting where the caption requires such named entities for meaningful summarization. As such, we propose the task of summarizing news video directly to entity-aware captions. We also release a large-scale dataset, VIEWS (VIdeo NEWS), to support research on this task. Further, we propose a method that augments visual information from videos with context retrieved from external world knowledge to generate entity-aware captions. We demonstrate the effectiveness of our approach on three video captioning models. We also show that our approach generalizes to existing news image captions dataset. With all the extensive experiments and insights, we believe we establish a solid basis for future research on this challenging task.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/beyond_grounding.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="ayyubi2024mmevents" class="col-sm-7">
    
      <span class="title">Beyond Grounding: Extracting Fine-Grained Event Hierarchies across Modalities</span>
      <span class="author">
        
          
            
	    
                <em>Hammad A. Ayyubi</em>,
              
            
          
        
          
            
	    
                
                  Christopher Thomas,
                
              
            
          
        
          
            
	    
                
                  Lovish Chum,
                
              
            
          
        
          
            
	    
                
                  Rahul Lokesh,
                
              
            
          
        
          
            
	    
                
                  Long Chen,
                
              
            
          
        
          
            
	    
                
                  Yulei Niu,
                
              
            
          
        
          
            
	    
                
                  Xudong Lin,
                
              
            
          
        
          
            
	    
                
                  Xuande Feng,
                
              
            
          
        
          
            
	    
                
                  Jaywon Koo,
                
              
            
          
        
          
            
	    
                
                  Sounak Ray,
                
              
            
          
        
          
            
	    
                
                  and Shih-Fu Chang
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>AAAI</em>
      
      
        2024
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/29718" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Events describe happenings in our world that are of importance. Naturally, understanding events mentioned in multimedia content and how they are related forms an important way of comprehending our world. Existing literature can infer if events across textual and visual (video) domains are identical (via grounding) and thus, on the same semantic level. However, grounding fails to capture the intricate cross-event relations that exist due to the same events being referred to on many semantic levels. For example, the abstract event of "war” manifests at a lower semantic level through subevents "tanks firing” (in video) and airplane "shot” (in text), leading to a hierarchical, multimodal relationship between the events. In this paper, we propose the task of extracting event hierarchies from multimodal (video and text) data to capture how the same event manifests itself in different modalities at different semantic levels. This reveals the structure of events and is critical to understanding them. To support research on this task, we introduce the Multimodal Hierarchical Events (MultiHiEve) dataset. Unlike prior video-language datasets, MultiHiEve is composed of news video-article pairs, which makes it rich in event hierarchies. We densely annotate a part of the dataset to construct the test benchmark. We show the limitations of state-of-the-art unimodal and multimodal baselines on this task. Further, we address these limitations via a new weakly supervised model, leveraging only unannotated video-article pairs from MultiHiEve. We perform a thorough evaluation of our proposed method which demonstrates improved performance on this task and highlight opportunities for future research. Data: https://github.com/hayyubi/multihieve</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/cara_2024.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="liu2024cara" class="col-sm-7">
    
      <span class="title">Detecting Multimodal Situations with Insufficient Context and Abstaining from Baseless Predictions</span>
      <span class="author">
        
          
            
	    
                
                  Junzhang Liu,
                
              
            
          
        
          
            
	    
                
                  Zhecan Wang,
                
              
            
          
        
          
            
	    
                <em>Hammad Ayyubi</em>,
              
            
          
        
          
            
	    
                
                  Haoxuan You,
                
              
            
          
        
          
            
	    
                
                  Chris Thomas,
                
              
            
          
        
          
            
	    
                
                  Rui Sun,
                
              
            
          
        
          
            
	    
                
                  Shih-Fu Chang,
                
              
            
          
        
          
            
	    
                
                  and Kai-Wei Chang
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>ACM MM</em>
      
      
        2024
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/pdf/2405.11145" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Despite the widespread adoption of Vision-Language Understanding (VLU) benchmarks such as VQA v2, OKVQA, A-OKVQA, GQA, VCR, SWAG, and VisualCOMET, our analysis reveals a pervasive issue affecting their integrity: these benchmarks contain samples where answers rely on assumptions unsupported by the provided context. Training models on such data foster biased learning and hallucinations as models tend to make similar unwarranted assumptions. To address this issue, we collect contextual data for each sample whenever available and train a context selection module to facilitate evidence-based model predictions. Strong improvements across multiple benchmarks demonstrate the effectiveness of our approach. Further, we develop a general-purpose Context-AwaRe Abstention (CARA) detector to identify samples lacking sufficient context and enhance model accuracy by abstaining from responding if the required context is absent. CARA exhibits generalization to new benchmarks it wasn’t trained on, underscoring its utility for future VLU benchmarks in detecting or cleaning samples with inadequate context. Finally, we curate a Context Ambiguity and Sufficiency Evaluation (CASE) set to benchmark the performance of insufficient context detectors. Overall, our work represents a significant advancement in ensuring that vision-language models generate trustworthy and evidence-based outputs in complex real-world scenarios.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/rap_2024.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="zare2024rap" class="col-sm-7">
    
      <span class="title">RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos</span>
      <span class="author">
        
          
            
	    
                
                  Ali Zare,
                
              
            
          
        
          
            
	    
                
                  Yulei Niu,
                
              
            
          
        
          
            
	    
                <em>Hammad Ayyubi</em>,
              
            
          
        
          
            
	    
                
                  and Shih-Fu Chang
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>ECCV</em>
      
      
        2024
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/pdf/2403.18600" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Procedure Planning in instructional videos entails generating a sequence of action steps based on visual observations of the initial and target states. Despite the rapid progress in this task, there remain several critical challenges to be solved: (1) Adaptive procedures: Prior works hold an unrealistic assumption that the number of action steps is known and fixed, leading to non-generalizable models in real-world scenarios where the sequence length varies. (2) Temporal relation: Understanding the step temporal relation knowledge is essential in producing reasonable and executable plans. (3) Annotation cost: Annotating instructional videos with step-level labels (i.e., timestamp) or sequence-level labels (i.e., action category) is demanding and labor-intensive, limiting its generalizability to large-scale datasets.In this work, we propose a new and practical setting, called adaptive procedure planning in instructional videos, where the procedure length is not fixed or pre-determined. To address these challenges we introduce Retrieval-Augmented Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively determines the conclusion of actions using an auto-regressive model architecture. For temporal relation, RAP establishes an external memory module to explicitly retrieve the most relevant state-action pairs from the training videos and revises the generated procedures. To tackle high annotation cost, RAP utilizes a weakly-supervised learning manner to expand the training dataset to other task-relevant, unannotated videos by generating pseudo labels for action steps. Experiments on CrossTask and COIN benchmarks show the  superiority of RAP over traditional fixed-length models, establishing it as a strong baseline solution for adaptive procedure planning.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/ayyubi_cur.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="ayyubi2023cur" class="col-sm-7">
    
      <span class="title">Learning from Children: Improving Image-Caption Pretraining via Curriculum</span>
      <span class="author">
        
          
            
	    
                <em>Hammad A. Ayyubi</em>,
              
            
          
        
          
            
	    
                
                  Rahul Lokesh,
                
              
            
          
        
          
            
	    
                
                  Alireza Zareian,
                
              
            
          
        
          
            
	    
                
                  Bo Wu,
                
              
            
          
        
          
            
	    
                
                  and Shih-Fu Chang
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>ACL Findings</em>
      
      
        2023
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2305.17540" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Image-caption pretraining has been quite successfully used for downstream vision tasks like zero-shot image classification and object detection. However, image-caption pretraining is still a hard problem – it requires multiple concepts (nouns) from captions to be aligned to several objects in images. To tackle this problem, we go to the roots – the best learner, children. We take inspiration from cognitive science studies dealing with children’s language learning to propose a curriculum learning framework. The learning begins with easy-to-align image caption pairs containing one concept per caption. The difficulty is progressively increased with each new phase by adding one more concept per caption. Correspondingly, the knowledge acquired in each learning phase is utilized in subsequent phases to effectively constrain the learning problem to aligning one new concept-object pair in each phase. We show that this learning strategy improves over vanilla image-caption training in various settings – pretraining from scratch, using a pretrained image or/and pretrained text encoder, low data regime etc.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/you_idealgpt.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="you2023idealgpt" class="col-sm-7">
    
      <span class="title">
IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models</span>
      <span class="author">
        
          
            
	    
                
                  Haoxuan You,
                
              
            
          
        
          
            
	    
                
                  Rui Sun,
                
              
            
          
        
          
            
	    
                
                  Zhecan Wang,
                
              
            
          
        
          
            
	    
                
                  Long Chen,
                
              
            
          
        
          
            
	    
                
                  Gengyu Wang,
                
              
            
          
        
          
            
	    
                <em>Hammad A. Ayyubi</em>,
              
            
          
        
          
            
	    
                
                  Kai-Wei Chang,
                
              
            
          
        
          
            
	    
                
                  and Shih-Fu Chang
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>EMNLP Findings</em>
      
      
        2023
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2305.14985" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE. Code is available at this https URL</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/chen_wstag.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="chen2022wstag" class="col-sm-7">
    
      <span class="title">Weakly-Supervised Temporal Article Grounding</span>
      <span class="author">
        
          
            
	    
                
                  Long Chen,
                
              
            
          
        
          
            
	    
                
                  Yulei Niu,
                
              
            
          
        
          
            
	    
                
                  Brian Chen,
                
              
            
          
        
          
            
	    
                
                  Xudong Lin,
                
              
            
          
        
          
            
	    
                
                  Guangxing Han,
                
              
            
          
        
          
            
	    
                
                  Christopher Thomas,
                
              
            
          
        
          
            
	    
                <em>Hammad Ayyubi</em>,
              
            
          
        
          
            
	    
                
                  Heng Ji,
                
              
            
          
        
          
            
	    
                
                  and Shih-Fu Chang
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>EMNLP</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2210.12444" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Given a long untrimmed video and natural language queries, video grounding (VG) aims to temporally localize the semantically-aligned video segments. Almost all existing VG work holds two simple but unrealistic assumptions: 1) All query sentences can be grounded in the corresponding video. 2) All query sentences for the same video are always at the same semantic scale. Unfortunately, both assumptions make today’s VG models fail to work in practice. For example, in real-world multimodal assets (eg, news articles), most of the sentences in the article can not be grounded in their affiliated videos, and they typically have rich hierarchical relations (ie, at different semantic scales). To this end, we propose a new challenging grounding task: Weakly-Supervised temporal Article Grounding (WSAG). Specifically, given an article and a relevant video, WSAG aims to localize all “groundable” sentences to the video, and these sentences are possibly at different semantic scales. Accordingly, we collect the first WSAG dataset to facilitate this task: YouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow articles and plentiful YouTube videos. In addition, we propose a simple but effective method DualMIL for WSAG, which consists of a two-level MIL loss and a single-/cross- sentence constraint loss. These training objectives are carefully designed for these relaxed assumptions. Extensive ablations have verified the effectiveness of DualMIL.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/pgode.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="ayyubi2019pgode" class="col-sm-7">
    
      <span class="title">Progressive Growing of Neural Ordinary Differential Equations</span>
      <span class="author">
        
          
            
	    
                <em>Hammad A. Ayyubi</em>,
              
            
          
        
          
            
	    
                
                  Yi Yao,
                
              
            
          
        
          
            
	    
                
                  and Ajay Divakaran
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>ICLR Workshop on Integration of Neural Networks and Differential Equations</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2003.03695" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Neural Ordinary Differential Equations (NODEs) have proven to be a powerful modeling tool for approximating (interpolation) and forecasting (extrapolation) irregularly sampled time series data. However, their performance degrades substantially when applied to real-world data, especially long-term data with complex behaviors (e.g., long-term trend across years, mid-term seasonality across months, and short-term local variation across days). To address the modeling of such complex data with different behaviors at different frequencies (time spans), we propose a novel progressive learning paradigm of NODEs for long-term time series forecasting. Specifically, following the principle of curriculum learning, we gradually increase the complexity of data and network capacity as training progresses. Our experiments with both synthetic data and real traffic data (PeMS Bay Area traffic data) show that our training methodology consistently improves the performance of vanilla NODEs by over 64%.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/msthesis.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="ayyubi2020msthesis" class="col-sm-7">
    
      <span class="title">Leveraging Human Reasoning to Understand and Improve Visual Question Answering</span>
      <span class="author">
        
          
	  
              <em>Hammad A. Ayyubi</em>
            
          
        
      </span>

      <span class="periodical">
      
        <em>UC San Diego, MS Thesis</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://escholarship.org/content/qt4462r6fv/qt4462r6fv.pdf" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Visual Question Answering (VQA) is the task of answering questions based on an image. The field has seen significant advances recently, with systems achieving high accuracy even on open-ended questions. However, a number of recent studies have shown that many of these advanced systems exploit biases in datasets, text of the question or similarity of images in the dataset.
    To study these reported biases, proposed approaches seek to identify areas of images or words of the questions as evidence that the model focuses on while answering questions. These mechanisms often tend to be limited as the model can answer incorrectly while focusing on the correct region of the image or vice versa.
    In this thesis, we seek to incorporate and leverage human reasoning to improve interpretability of these VQA models. Essentially, we train models to generate human-like language as evidence or reasons/rationales for the answers that they predict. Further, we show that this type of system has the potential to improve the accuracy on VQA task itself as well.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/dynamicrec.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="tanjim2020dynamicrec" class="col-sm-7">
    
      <span class="title">DynamicRec: A Dynamic Convolutional Network for Next Item Recommendation</span>
      <span class="author">
        
          
            
	    
                
                  Md Mehrab Tanjim,
                
              
            
          
        
          
            
	    
                <em>Hammad A. Ayyubi</em>,
              
            
          
        
          
            
	    
                
                  and Garrison W Cottrell
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>Proceedings of the 29th ACM International Conference on Information and Knowledge Management</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://dl.acm.org/doi/abs/10.1145/3340531.3412118" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Recently convolutional networks have shown significant promise for modeling sequential user interactions for recommendations. Critically, such networks rely on fixed convolutional kernels to capture sequential behavior. In this paper, we argue that all the dynamics of the item-to-item transition in session-based settings may not be observable at training time. Hence we propose DynamicRec, which uses dynamic convolutions to compute the convolutional kernels on the fly based on the current input. We show through experiments that this approach significantly outperforms existing convolutional models on real datasets in session-based settings.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/genrat.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="ayyubi2019genrat" class="col-sm-7">
    
      <span class="title">Generating Rationale in Visual Question Answering</span>
      <span class="author">
        
          
            
	    
                <em>Hammad A. Ayyubi*</em>,
              
            
          
        
          
            
	    
                
                  Md. Mehrab Tanjim*,
                
              
            
          
        
          
            
	    
                
                  Julian McAuley,
                
              
            
          
        
          
            
	    
                
                  and Garrison W. Cottrell
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv:2004.02032</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2004.02032" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Despite recent advances in Visual Question Answering (VQA), it remains a challenge to determine how much success can be attributed to sound reasoning and comprehension ability. We seek to investigate this question by proposing a new task of rationale generation. Essentially, we task a VQA model with generating rationales for the answers it predicts. We use data from the Visual Commonsense Reasoning (VCR) task, as it contains ground-truth rationales along with visual questions and answers. We first investigate commonsense understanding in one of the leading VCR models, ViLBERT, by generating rationales from pretrained weights using a state-of-the-art language model, GPT-2. Next, we seek to jointly train ViLBERT with GPT-2 in an end-to-end fashion with the dual task of predicting the answer in VQA and generating rationales. We show that this kind of training injects commonsense understanding in the VQA model through quantitative and qualitative evaluation metrics.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3 abbr">
  
  <img src="/assets/img/ganspection.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
  
  </div>

  <div id="ayyubi2019ganspection" class="col-sm-7">
    
      <span class="title">GANspection</span>
      <span class="author">
        
          
	  
              <em>Hammad A. Ayyubi</em>
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv:1910.09638</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/1910.09638" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Generative Adversarial Networks (GANs) have been used extensively and quite successfully for unsupervised learning. As GANs don’t approximate an explicit probability distribution, it’s an interesting study to inspect the latent space representations learned by GANs. The current work seeks to push the boundaries of such inspection methods to further understand in more detail the manifold being learned by GANs. Various interpolation and extrapolation techniques along with vector arithmetic is used to understand the learned manifold. We show through experiments that GANs indeed learn a data probability distribution rather than memorize images/data. Further, we prove that GANs encode semantically relevant information in the learned probability distribution. The experiments have been performed on two publicly available datasets - Large Scale Scene Understanding (LSUN) and CelebA.</p>
    </span>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2024 Hammad Ayyubi.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    Last updated: October 16, 2024.
    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>







</html>
