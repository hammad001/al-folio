---
---

@string{aps = {American Physical Society,}}

@article{ayyubi2019pgode,
    abbr={pgode.png},
    title={Progressive Growing of {Neural Ordinary Differential Equations}},
    author={Hammad A. Ayyubi and Yi Yao and Ajay Divakaran},
    journal={ICLR Workshop on Integration of Neural Networks and Differential Equations},
    year={2020},
    url={https://arxiv.org/abs/2003.03695},
    html={https://arxiv.org/abs/2003.03695},
    abstract={Neural Ordinary Differential Equations (NODEs) have proven to be a powerful modeling tool for approximating (interpolation) and forecasting (extrapolation) irregularly sampled time series data. However, their performance degrades substantially when applied to real-world data, especially long-term data with complex behaviors (e.g., long-term trend across years, mid-term seasonality across months, and short-term local variation across days). To address the modeling of such complex data with different behaviors at different frequencies (time spans), we propose a novel progressive learning paradigm of NODEs for long-term time series forecasting. Specifically, following the principle of curriculum learning, we gradually increase the complexity of data and network capacity as training progresses. Our experiments with both synthetic data and real traffic data (PeMS Bay Area traffic data) show that our training methodology consistently improves the performance of vanilla NODEs by over 64\%.} 
}

@article{ayyubi2019genrat,
    abbr={genrat.png},
    title={Generating Rationale in {Visual Question Answering}},
    author={Hammad A. Ayyubi* and Md. Mehrab Tanjim* and Julian McAuley and Garrison W. Cottrell},
    journal={arXiv:2004.02032},
    year={2019},
    url={https://arxiv.org/abs/2004.02032},
    html={https://arxiv.org/abs/2004.02032},
    abstract={Despite recent advances in Visual Question Answering (VQA), it remains a challenge to determine how much success can be attributed to sound reasoning and comprehension ability. We seek to investigate this question by proposing a new task of rationale generation. Essentially, we task a VQA model with generating rationales for the answers it predicts. We use data from the Visual Commonsense Reasoning (VCR) task, as it contains ground-truth rationales along with visual questions and answers. We first investigate commonsense understanding in one of the leading VCR models, ViLBERT, by generating rationales from pretrained weights using a state-of-the-art language model, GPT-2. Next, we seek to jointly train ViLBERT with GPT-2 in an end-to-end fashion with the dual task of predicting the answer in VQA and generating rationales. We show that this kind of training injects commonsense understanding in the VQA model through quantitative and qualitative evaluation metrics.}
}

@article{ayyubi2019ganspection,
    abbr={ganspection.png},
    title={GANspection},
    author={Hammad A. Ayyubi},
    year={2019},
    journal={arXiv:1910.09638},
    url={https://arxiv.org/abs/1910.09638},
    html={https://arxiv.org/abs/1910.09638},
    abstract={Generative Adversarial Networks (GANs) have been used extensively and quite successfully for unsupervised learning. As GANs don't approximate an explicit probability distribution, it's an interesting study to inspect the latent space representations learned by GANs. The current work seeks to push the boundaries of such inspection methods to further understand in more detail the manifold being learned by GANs. Various interpolation and extrapolation techniques along with vector arithmetic is used to understand the learned manifold. We show through experiments that GANs indeed learn a data probability distribution rather than memorize images/data. Further, we prove that GANs encode semantically relevant information in the learned probability distribution. The experiments have been performed on two publicly available datasets - Large Scale Scene Understanding (LSUN) and CelebA.}
}

@article{ayyubi2020msthesis,
    abbr={msthesis.png},
    title={Leveraging Human Reasoning to Understand and Improve Visual Question Answering},
    author={Hammad A. Ayyubi},
    year={2020},
    journal={UC San Diego, MS Thesis},
    url={https://escholarship.org/content/qt4462r6fv/qt4462r6fv.pdf},
    html={https://escholarship.org/content/qt4462r6fv/qt4462r6fv.pdf},
    abstract={Visual Question Answering (VQA) is the task of answering questions based on an image. The field has seen significant advances recently, with systems achieving high accuracy even on open-ended questions. However, a number of recent studies have shown that many of these advanced systems exploit biases in datasets, text of the question or similarity of images in the dataset.
    To study these reported biases, proposed approaches seek to identify areas of images or words of the questions as evidence that the model focuses on while answering questions. These mechanisms often tend to be limited as the model can answer incorrectly while focusing on the correct region of the image or vice versa.
    In this thesis, we seek to incorporate and leverage human reasoning to improve interpretability of these VQA models. Essentially, we train models to generate human-like language as evidence or reasons/rationales for the answers that they predict. Further, we show that this type of system has the potential to improve the accuracy on VQA task itself as well.}
}

@article{tanjim2020dynamicrec,
    abbr={dynamicrec.png},
    title={DynamicRec: A Dynamic Convolutional Network for Next Item Recommendation},
    author={Md Mehrab Tanjim and Hammad A. Ayyubi and Garrison W Cottrell},
    year={2020},
    journal={Proceedings of the 29th ACM International Conference on Information and Knowledge Management},
    url={https://dl.acm.org/doi/abs/10.1145/3340531.3412118},
    html={https://dl.acm.org/doi/abs/10.1145/3340531.3412118},
    abstract={Recently convolutional networks have shown significant promise for modeling sequential user interactions for recommendations. Critically, such networks rely on fixed convolutional kernels to capture sequential behavior. In this paper, we argue that all the dynamics of the item-to-item transition in session-based settings may not be observable at training time. Hence we propose DynamicRec, which uses dynamic convolutions to compute the convolutional kernels on the fly based on the current input. We show through experiments that this approach significantly outperforms existing convolutional models on real datasets in session-based settings.}
}

@article{chen2022wstag,
    abbr={chen_wstag.png},
    title={Weakly-Supervised Temporal Article Grounding},
    author={Long Chen and Yulei Niu and Brian Chen and Xudong Lin and Guangxing Han and Christopher Thomas and Hammad Ayyubi and Heng Ji and Shih-Fu Chang},
    year={2022},
    journal={EMNLP},
    url={https://arxiv.org/abs/2210.12444},
    html={https://arxiv.org/abs/2210.12444},
    abstract={Given a long untrimmed video and natural language queries, video grounding (VG) aims to temporally localize the semantically-aligned video segments. Almost all existing VG work holds two simple but unrealistic assumptions: 1) All query sentences can be grounded in the corresponding video. 2) All query sentences for the same video are always at the same semantic scale. Unfortunately, both assumptions make today's VG models fail to work in practice. For example, in real-world multimodal assets (eg, news articles), most of the sentences in the article can not be grounded in their affiliated videos, and they typically have rich hierarchical relations (ie, at different semantic scales). To this end, we propose a new challenging grounding task: Weakly-Supervised temporal Article Grounding (WSAG). Specifically, given an article and a relevant video, WSAG aims to localize all ``groundable'' sentences to the video, and these sentences are possibly at different semantic scales. Accordingly, we collect the first WSAG dataset to facilitate this task: YouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow articles and plentiful YouTube videos. In addition, we propose a simple but effective method DualMIL for WSAG, which consists of a two-level MIL loss and a single-/cross- sentence constraint loss. These training objectives are carefully designed for these relaxed assumptions. Extensive ablations have verified the effectiveness of DualMIL.}
}

@article{ayyubi2023cur,
    abbr={ayyubi_cur.png},
    title={Learning from Children: Improving Image-Caption Pretraining via Curriculum},
    author={Hammad A. Ayyubi and Rahul Lokesh and Alireza Zareian and Bo Wu and Shih-Fu Chang},
    year={2023},
    journal={ACL Findings},
    url={https://arxiv.org/abs/2305.17540},
    html={https://arxiv.org/abs/2305.17540},
    abstract={Image-caption pretraining has been quite successfully used for downstream vision tasks like zero-shot image classification and object detection. However, image-caption pretraining is still a hard problem -- it requires multiple concepts (nouns) from captions to be aligned to several objects in images. To tackle this problem, we go to the roots -- the best learner, children. We take inspiration from cognitive science studies dealing with children's language learning to propose a curriculum learning framework. The learning begins with easy-to-align image caption pairs containing one concept per caption. The difficulty is progressively increased with each new phase by adding one more concept per caption. Correspondingly, the knowledge acquired in each learning phase is utilized in subsequent phases to effectively constrain the learning problem to aligning one new concept-object pair in each phase. We show that this learning strategy improves over vanilla image-caption training in various settings -- pretraining from scratch, using a pretrained image or/and pretrained text encoder, low data regime etc.}
}

@article{you2023idealgpt,
    abbr={you_idealgpt.png},
    title={
IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models},
    author={Haoxuan You and Rui Sun and Zhecan Wang and Long Chen and Gengyu Wang and Hammad A. Ayyubi and Kai-Wei Chang and Shih-Fu Chang},
    year={2023},
    journal={EMNLP Findings},
    url={https://arxiv.org/abs/2305.14985},
    html={https://arxiv.org/abs/2305.14985},
    abstract={The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE. Code is available at this https URL}
}

@article{wang2024journey,
    abbr={journey_bench.png},
    title={JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images},
    author={Zhecan Wang and Junzhang Liu and Chia-Wei Tang and Hani Alomari and Anushka Sivakumar and Rui Sun and Wenhao Li and Md Atabuzzaman and Hammad Ayyubi and Haoxuan You and Alvi Ishmam and Kai-Wei Chang and Shih-Fu Chang and Chris Thomas},
    journal={NeurIPS},
    year={2024},
    url={https://arxiv.org/pdf/2409.12953},
    html={https://arxiv.org/pdf/2409.12953},
    abstract={Existing vision-language understanding benchmarks largely consist of images of objects in their usual contexts. As a consequence, recent multimodal large language models can perform well with only a shallow visual understanding by relying on background language biases. Thus, strong performance on these benchmarks does not necessarily correlate with strong visual understanding. In this paper, we release JourneyBench, a comprehensive human-annotated benchmark of generated images designed to assess the model's fine-grained multimodal reasoning abilities across five tasks: complementary multimodal chain of thought, multi-image VQA, imaginary image captioning, VQA with hallucination triggers, and fine-grained retrieval with sample-specific distractors. Unlike existing benchmarks, JourneyBench explicitly requires fine-grained multimodal reasoning in unusual imaginary scenarios where language bias and holistic image gist are insufficient. We benchmark state-of-the-art models on JourneyBench and analyze performance along a number of fine-grained dimensions. Results across all five tasks show that JourneyBench is exceptionally challenging for even the best models, indicating that models' visual reasoning abilities are not as strong as they first appear. We discuss the implications of our findings and propose avenues for further research.}
}

@article{ayyubi2023videoentitysum,
    abbr={video_entity_sum.png},
    title={
Video Summarization: Towards Entity-Aware Captions},
    author={Hammad A. Ayyubi and Tianqi Liu and Arsha Nagrani and Xudong Lin and Mingda Zhang and Anurag Arnab and Feng Han and Yukun Zhu and Jialu Liu and Shih-Fu Chang},
    year={2024},
    journal={EMNLP},
    url={https://arxiv.org/abs/2312.02188},
    html={https://arxiv.org/abs/2312.02188},
    abstract={Existing popular video captioning benchmarks and models deal with generic captions devoid of specific person, place or organization named entities. In contrast, news videos present a challenging setting where the caption requires such named entities for meaningful summarization. As such, we propose the task of summarizing news video directly to entity-aware captions. We also release a large-scale dataset, VIEWS (VIdeo NEWS), to support research on this task. Further, we propose a method that augments visual information from videos with context retrieved from external world knowledge to generate entity-aware captions. We demonstrate the effectiveness of our approach on three video captioning models. We also show that our approach generalizes to existing news image captions dataset. With all the extensive experiments and insights, we believe we establish a solid basis for future research on this challenging task.}
}

@article{ayyubi2024mmevents,
    abbr={beyond_grounding.png},
    title={Beyond Grounding: Extracting Fine-Grained Event Hierarchies across Modalities},
    author={Hammad A. Ayyubi and Christopher Thomas and Lovish Chum and Rahul Lokesh and Long Chen and Yulei Niu and Xudong Lin and Xuande Feng and Jaywon Koo and Sounak Ray and Shih-Fu Chang},
    year={2024},
    journal={AAAI},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/29718},
    html={https://ojs.aaai.org/index.php/AAAI/article/view/29718},
    abstract={Events describe happenings in our world that are of importance. Naturally, understanding events mentioned in multimedia content and how they are related forms an important way of comprehending our world. Existing literature can infer if events across textual and visual (video) domains are identical (via grounding) and thus, on the same semantic level. However, grounding fails to capture the intricate cross-event relations that exist due to the same events being referred to on many semantic levels. For example, the abstract event of "war'' manifests at a lower semantic level through subevents "tanks firing'' (in video) and airplane "shot'' (in text), leading to a hierarchical, multimodal relationship between the events. In this paper, we propose the task of extracting event hierarchies from multimodal (video and text) data to capture how the same event manifests itself in different modalities at different semantic levels. This reveals the structure of events and is critical to understanding them. To support research on this task, we introduce the Multimodal Hierarchical Events (MultiHiEve) dataset. Unlike prior video-language datasets, MultiHiEve is composed of news video-article pairs, which makes it rich in event hierarchies. We densely annotate a part of the dataset to construct the test benchmark. We show the limitations of state-of-the-art unimodal and multimodal baselines on this task. Further, we address these limitations via a new weakly supervised model, leveraging only unannotated video-article pairs from MultiHiEve. We perform a thorough evaluation of our proposed method which demonstrates improved performance on this task and highlight opportunities for future research. Data: https://github.com/hayyubi/multihieve}
}

@article{liu2024cara,
    abbr={cara_2024.png},
    title={Detecting Multimodal Situations with Insufficient Context and Abstaining from Baseless Predictions},
    author={Junzhang Liu and Zhecan Wang and Hammad Ayyubi and Haoxuan You and Chris Thomas and Rui Sun and Shih-Fu Chang and Kai-Wei Chang},
    year={2024},
    journal={ACM MM},
    url={https://arxiv.org/pdf/2405.11145},
    html={https://arxiv.org/pdf/2405.11145},
    abstract={Despite the widespread adoption of Vision-Language Understanding (VLU) benchmarks such as VQA v2, OKVQA, A-OKVQA, GQA, VCR, SWAG, and VisualCOMET, our analysis reveals a pervasive issue affecting their integrity: these benchmarks contain samples where answers rely on assumptions unsupported by the provided context. Training models on such data foster biased learning and hallucinations as models tend to make similar unwarranted assumptions. To address this issue, we collect contextual data for each sample whenever available and train a context selection module to facilitate evidence-based model predictions. Strong improvements across multiple benchmarks demonstrate the effectiveness of our approach. Further, we develop a general-purpose Context-AwaRe Abstention (CARA) detector to identify samples lacking sufficient context and enhance model accuracy by abstaining from responding if the required context is absent. CARA exhibits generalization to new benchmarks it wasn't trained on, underscoring its utility for future VLU benchmarks in detecting or cleaning samples with inadequate context. Finally, we curate a Context Ambiguity and Sufficiency Evaluation (CASE) set to benchmark the performance of insufficient context detectors. Overall, our work represents a significant advancement in ensuring that vision-language models generate trustworthy and evidence-based outputs in complex real-world scenarios.}
}

@article{zare2024rap,
    abbr={rap_2024.png},
    title={RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos},
    author={Ali Zare and Yulei Niu and Hammad Ayyubi and Shih-Fu Chang},
    year={2024},
    journal={ECCV},
    url={https://arxiv.org/pdf/2403.18600},
    html={https://arxiv.org/pdf/2403.18600},
    abstract={Procedure Planning in instructional videos entails generating a sequence of action steps based on visual observations of the initial and target states. Despite the rapid progress in this task, there remain several critical challenges to be solved: (1) Adaptive procedures: Prior works hold an unrealistic assumption that the number of action steps is known and fixed, leading to non-generalizable models in real-world scenarios where the sequence length varies. (2) Temporal relation: Understanding the step temporal relation knowledge is essential in producing reasonable and executable plans. (3) Annotation cost: Annotating instructional videos with step-level labels (i.e., timestamp) or sequence-level labels (i.e., action category) is demanding and labor-intensive, limiting its generalizability to large-scale datasets.In this work, we propose a new and practical setting, called adaptive procedure planning in instructional videos, where the procedure length is not fixed or pre-determined. To address these challenges we introduce Retrieval-Augmented Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively determines the conclusion of actions using an auto-regressive model architecture. For temporal relation, RAP establishes an external memory module to explicitly retrieve the most relevant state-action pairs from the training videos and revises the generated procedures. To tackle high annotation cost, RAP utilizes a weakly-supervised learning manner to expand the training dataset to other task-relevant, unannotated videos by generating pseudo labels for action steps. Experiments on CrossTask and COIN benchmarks show the  superiority of RAP over traditional fixed-length models, establishing it as a strong baseline solution for adaptive procedure planning.}
}
