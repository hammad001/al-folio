---
---

@string{aps = {American Physical Society,}}

@article{ayyubi2019pgode,
    abbr={pgode.png},
    title={Progressive Growing of {Neural Ordinary Differential Equations}},
    author={Hammad A. Ayyubi and Yi Yao and Ajay Divakaran},
    journal={ICLR Workshop on Integration of Neural Networks and Differential Equations},
    year={2020},
    url={https://arxiv.org/abs/2003.03695},
    html={https://arxiv.org/abs/2003.03695},
    abstract={Neural Ordinary Differential Equations (NODEs) have proven to be a powerful modeling tool for approximating (interpolation) and forecasting (extrapolation) irregularly sampled time series data. However, their performance degrades substantially when applied to real-world data, especially long-term data with complex behaviors (e.g., long-term trend across years, mid-term seasonality across months, and short-term local variation across days). To address the modeling of such complex data with different behaviors at different frequencies (time spans), we propose a novel progressive learning paradigm of NODEs for long-term time series forecasting. Specifically, following the principle of curriculum learning, we gradually increase the complexity of data and network capacity as training progresses. Our experiments with both synthetic data and real traffic data (PeMS Bay Area traffic data) show that our training methodology consistently improves the performance of vanilla NODEs by over 64\%.} 
}

@article{ayyubi2019genrat,
    abbr={genrat.png},
    title={Generating Rationale in {Visual Question Answering}},
    author={Hammad A. Ayyubi* and Md. Mehrab Tanjim* and Julian McAuley and Garrison W. Cottrell},
    journal={arXiv:2004.02032},
    year={2019},
    url={https://arxiv.org/abs/2004.02032},
    html={https://arxiv.org/abs/2004.02032},
    abstract={Despite recent advances in Visual Question Answering (VQA), it remains a challenge to determine how much success can be attributed to sound reasoning and comprehension ability. We seek to investigate this question by proposing a new task of rationale generation. Essentially, we task a VQA model with generating rationales for the answers it predicts. We use data from the Visual Commonsense Reasoning (VCR) task, as it contains ground-truth rationales along with visual questions and answers. We first investigate commonsense understanding in one of the leading VCR models, ViLBERT, by generating rationales from pretrained weights using a state-of-the-art language model, GPT-2. Next, we seek to jointly train ViLBERT with GPT-2 in an end-to-end fashion with the dual task of predicting the answer in VQA and generating rationales. We show that this kind of training injects commonsense understanding in the VQA model through quantitative and qualitative evaluation metrics.}
}

@article{ayyubi2019ganspection,
    abbr={ganspection.png},
    title={GANspection},
    author={Hammad A. Ayyubi},
    year={2019},
    journal={arXiv:1910.09638},
    url={https://arxiv.org/abs/1910.09638},
    html={https://arxiv.org/abs/1910.09638},
    abstract={Generative Adversarial Networks (GANs) have been used extensively and quite successfully for unsupervised learning. As GANs don't approximate an explicit probability distribution, it's an interesting study to inspect the latent space representations learned by GANs. The current work seeks to push the boundaries of such inspection methods to further understand in more detail the manifold being learned by GANs. Various interpolation and extrapolation techniques along with vector arithmetic is used to understand the learned manifold. We show through experiments that GANs indeed learn a data probability distribution rather than memorize images/data. Further, we prove that GANs encode semantically relevant information in the learned probability distribution. The experiments have been performed on two publicly available datasets - Large Scale Scene Understanding (LSUN) and CelebA.}
}

@article{ayyubi2020msthesis,
    abbr={msthesis.png},
    title={Leveraging Human Reasoning to Understand and Improve Visual Question Answering},
    author={Hammad A. Ayyubi},
    year={2020},
    journal={UC San Diego, MS Thesis},
    url={https://escholarship.org/content/qt4462r6fv/qt4462r6fv.pdf},
    html={https://escholarship.org/content/qt4462r6fv/qt4462r6fv.pdf},
    abstract={Visual Question Answering (VQA) is the task of answering questions based on an image. The field has seen significant advances recently, with systems achieving high accuracy even on open-ended questions. However, a number of recent studies have shown that many of these advanced systems exploit biases in datasets, text of the question or similarity of images in the dataset.
    To study these reported biases, proposed approaches seek to identify areas of images or words of the questions as evidence that the model focuses on while answering questions. These mechanisms often tend to be limited as the model can answer incorrectly while focusing on the correct region of the image or vice versa.
    In this thesis, we seek to incorporate and leverage human reasoning to improve interpretability of these VQA models. Essentially, we train models to generate human-like language as evidence or reasons/rationales for the answers that they predict. Further, we show that this type of system has the potential to improve the accuracy on VQA task itself as well.}
}

@article{tanjim2020dynamicrec,
    abbr={dynamicrec.png},
    title={DynamicRec: A Dynamic Convolutional Network for Next Item Recommendation},
    author={Md Mehrab Tanjim and Hammad A. Ayyubi and Garrison W Cottrell},
    year={2020},
    journal={Proceedings of the 29th ACM International Conference on Information and Knowledge Management},
    url={https://dl.acm.org/doi/abs/10.1145/3340531.3412118},
    html={https://dl.acm.org/doi/abs/10.1145/3340531.3412118},
    abstract={Recently convolutional networks have shown significant promise for modeling sequential user interactions for recommendations. Critically, such networks rely on fixed convolutional kernels to capture sequential behavior. In this paper, we argue that all the dynamics of the item-to-item transition in session-based settings may not be observable at training time. Hence we propose DynamicRec, which uses dynamic convolutions to compute the convolutional kernels on the fly based on the current input. We show through experiments that this approach significantly outperforms existing convolutional models on real datasets in session-based settings.}
}
