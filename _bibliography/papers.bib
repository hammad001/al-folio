---
---

@string{aps = {American Physical Society,}}

@article{ayyubi2019pgode,
    abbr={pgode.png},
    title={Progressive Growing of {Neural Ordinary Differential Equations}},
    author={Hammad A. Ayyubi and Yi Yao and Ajay Divakaran},
    journal={ICLR Workshop on Integration of Neural Networks and Differential Equations},
    year={2020},
    url={https://arxiv.org/abs/2003.03695},
    html={https://arxiv.org/abs/2003.03695},
    abstract={Neural Ordinary Differential Equations (NODEs) have proven to be a powerful modeling tool for approximating (interpolation) and forecasting (extrapolation) irregularly sampled time series data. However, their performance degrades substantially when applied to real-world data, especially long-term data with complex behaviors (e.g., long-term trend across years, mid-term seasonality across months, and short-term local variation across days). To address the modeling of such complex data with different behaviors at different frequencies (time spans), we propose a novel progressive learning paradigm of NODEs for long-term time series forecasting. Specifically, following the principle of curriculum learning, we gradually increase the complexity of data and network capacity as training progresses. Our experiments with both synthetic data and real traffic data (PeMS Bay Area traffic data) show that our training methodology consistently improves the performance of vanilla NODEs by over 64\%.} 
}

@article{ayyubi2019genrat,
    abbr={genrat.png},
    title={Generating Rationale in {Visual Question Answering}},
    author={Hammad A. Ayyubi* and Md. Mehrab Tanjim* and Julian McAuley and Garrison W. Cottrell},
    journal={arXiv:2004.02032},
    year={2019},
    url={https://arxiv.org/abs/2004.02032},
    html={https://arxiv.org/abs/2004.02032},
    abstract={Despite recent advances in Visual Question Answering (VQA), it remains a challenge to determine how much success can be attributed to sound reasoning and comprehension ability. We seek to investigate this question by proposing a new task of rationale generation. Essentially, we task a VQA model with generating rationales for the answers it predicts. We use data from the Visual Commonsense Reasoning (VCR) task, as it contains ground-truth rationales along with visual questions and answers. We first investigate commonsense understanding in one of the leading VCR models, ViLBERT, by generating rationales from pretrained weights using a state-of-the-art language model, GPT-2. Next, we seek to jointly train ViLBERT with GPT-2 in an end-to-end fashion with the dual task of predicting the answer in VQA and generating rationales. We show that this kind of training injects commonsense understanding in the VQA model through quantitative and qualitative evaluation metrics.}
}

@article{ayyubi2019ganspection,
    abbr={ganspection.png},
    title={GANspection},
    author={Hammad A. Ayyubi},
    year={2019},
    journal={arXiv:1910.09638},
    url={https://arxiv.org/abs/1910.09638},
    html={https://arxiv.org/abs/1910.09638},
    abstract={Generative Adversarial Networks (GANs) have been used extensively and quite successfully for unsupervised learning. As GANs don't approximate an explicit probability distribution, it's an interesting study to inspect the latent space representations learned by GANs. The current work seeks to push the boundaries of such inspection methods to further understand in more detail the manifold being learned by GANs. Various interpolation and extrapolation techniques along with vector arithmetic is used to understand the learned manifold. We show through experiments that GANs indeed learn a data probability distribution rather than memorize images/data. Further, we prove that GANs encode semantically relevant information in the learned probability distribution. The experiments have been performed on two publicly available datasets - Large Scale Scene Understanding (LSUN) and CelebA.}
}

@article{ayyubi2020msthesis,
    abbr={msthesis.png},
    title={Leveraging Human Reasoning to Understand and Improve Visual Question Answering},
    author={Hammad A. Ayyubi},
    year={2020},
    journal={UC San Diego, MS Thesis},
    url={https://escholarship.org/content/qt4462r6fv/qt4462r6fv.pdf},
    html={https://escholarship.org/content/qt4462r6fv/qt4462r6fv.pdf},
    abstract={Visual Question Answering (VQA) is the task of answering questions based on an image. The field has seen significant advances recently, with systems achieving high accuracy even on open-ended questions. However, a number of recent studies have shown that many of these advanced systems exploit biases in datasets, text of the question or similarity of images in the dataset.
    To study these reported biases, proposed approaches seek to identify areas of images or words of the questions as evidence that the model focuses on while answering questions. These mechanisms often tend to be limited as the model can answer incorrectly while focusing on the correct region of the image or vice versa.
    In this thesis, we seek to incorporate and leverage human reasoning to improve interpretability of these VQA models. Essentially, we train models to generate human-like language as evidence or reasons/rationales for the answers that they predict. Further, we show that this type of system has the potential to improve the accuracy on VQA task itself as well.}
}

@article{tanjim2020dynamicrec,
    abbr={dynamicrec.png},
    title={DynamicRec: A Dynamic Convolutional Network for Next Item Recommendation},
    author={Md Mehrab Tanjim and Hammad A. Ayyubi and Garrison W Cottrell},
    year={2020},
    journal={Proceedings of the 29th ACM International Conference on Information and Knowledge Management},
    url={https://dl.acm.org/doi/abs/10.1145/3340531.3412118},
    html={https://dl.acm.org/doi/abs/10.1145/3340531.3412118},
    abstract={Recently convolutional networks have shown significant promise for modeling sequential user interactions for recommendations. Critically, such networks rely on fixed convolutional kernels to capture sequential behavior. In this paper, we argue that all the dynamics of the item-to-item transition in session-based settings may not be observable at training time. Hence we propose DynamicRec, which uses dynamic convolutions to compute the convolutional kernels on the fly based on the current input. We show through experiments that this approach significantly outperforms existing convolutional models on real datasets in session-based settings.}
}

@article{ayyubi2022mmevents,
    abbr={ayyubi_mmevents.png},
    title={Multimodal Event Graphs: Towards Event Centric Understanding of Multimodal World},
    author={Hammad A. Ayyubi and Christopher Thomas and Lovish Chum and Rahul Lokesh and Yulei Niu and Xudong Lin and Long Chen and Jaywon Koo and Sounak Ray and Shih-Fu Chang},
    year={2022},
    journal={arXiv:2206.07207},
    url={https://arxiv.org/pdf/2206.07207v1.pdf},
    html={https://arxiv.org/pdf/2206.07207v1.pdf},
    abstract={Understanding how events described or shown in multimedia content relate to one another is a critical component to developing robust artificially intelligent systems which can reason about real-world media. While much research has been devoted to event understanding in the text, image, and video domains, none have explored the complex relations that events experience across domains. For example, a news article may describe a `protest' event while a video shows an `arrest' event. Recognizing that the visual `arrest' event is a subevent of the broader `protest' event is a challenging, yet important problem that prior work has not explored. In this paper, we propose the novel task of MultiModal Event Event Relations to recognize such cross-modal event relations. We contribute a large-scale dataset consisting of 100k video-news article pairs, as well as a benchmark of densely annotated data. We also propose a weakly supervised multimodal method which integrates commonsense knowledge from an external knowledge base (KB) to predict rich multimodal event hierarchies. Experiments show that our model outperforms a number of competitive baselines on our proposed benchmark. We also perform a detailed analysis of our model's performance and suggest directions for future research.}
}

@article{chen2022wstag,
    abbr={chen_wstag.png},
    title={Weakly-Supervised Temporal Article Grounding},
    author={Long Chen and Yulei Niu and Brian Chen and Xudong Lin and Guangxing Han and Christopher Thomas and Hammad Ayyubi and Heng Ji and Shih-Fu Chang},
    year={2022},
    journal={EMNLP},
    url={https://arxiv.org/abs/2210.12444},
    html={https://arxiv.org/abs/2210.12444},
    abstract={Given a long untrimmed video and natural language queries, video grounding (VG) aims to temporally localize the semantically-aligned video segments. Almost all existing VG work holds two simple but unrealistic assumptions: 1) All query sentences can be grounded in the corresponding video. 2) All query sentences for the same video are always at the same semantic scale. Unfortunately, both assumptions make today's VG models fail to work in practice. For example, in real-world multimodal assets (eg, news articles), most of the sentences in the article can not be grounded in their affiliated videos, and they typically have rich hierarchical relations (ie, at different semantic scales). To this end, we propose a new challenging grounding task: Weakly-Supervised temporal Article Grounding (WSAG). Specifically, given an article and a relevant video, WSAG aims to localize all ``groundable'' sentences to the video, and these sentences are possibly at different semantic scales. Accordingly, we collect the first WSAG dataset to facilitate this task: YouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow articles and plentiful YouTube videos. In addition, we propose a simple but effective method DualMIL for WSAG, which consists of a two-level MIL loss and a single-/cross- sentence constraint loss. These training objectives are carefully designed for these relaxed assumptions. Extensive ablations have verified the effectiveness of DualMIL.}
}

@article{ayyubi2023cur,
    abbr={ayyubi_cur.png},
    title={Learning from Children: Improving Image-Caption Pretraining via Curriculum},
    author={Hammad A. Ayyubi and Rahul Lokesh and Alireza Zareian and Bo Wu and Shih-Fu Chang},
    year={2023},
    journal={ACL Findings},
    url={https://arxiv.org/abs/2305.17540},
    html={https://arxiv.org/abs/2305.17540},
    abstract={Image-caption pretraining has been quite successfully used for downstream vision tasks like zero-shot image classification and object detection. However, image-caption pretraining is still a hard problem -- it requires multiple concepts (nouns) from captions to be aligned to several objects in images. To tackle this problem, we go to the roots -- the best learner, children. We take inspiration from cognitive science studies dealing with children's language learning to propose a curriculum learning framework. The learning begins with easy-to-align image caption pairs containing one concept per caption. The difficulty is progressively increased with each new phase by adding one more concept per caption. Correspondingly, the knowledge acquired in each learning phase is utilized in subsequent phases to effectively constrain the learning problem to aligning one new concept-object pair in each phase. We show that this learning strategy improves over vanilla image-caption training in various settings -- pretraining from scratch, using a pretrained image or/and pretrained text encoder, low data regime etc.}
}

@article{you2023idealgpt,
    abbr={you_idealgpt.png},
    title={
IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models},
    author={Haoxuan You and Rui Sun and Zhecan Wang and Long Chen and Gengyu Wang and Hammad A. Ayyubi and Kai-Wei Chang and Shih-Fu Chang},
    year={2023},
    journal={arXiv:2305.14985},
    url={https://arxiv.org/abs/2305.14985},
    html={https://arxiv.org/abs/2305.14985},
    abstract={The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE. Code is available at this https URL}
}
